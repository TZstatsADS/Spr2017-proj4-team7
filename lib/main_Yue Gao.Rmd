---
title: "Project 4 - SVM"
author: "Yue Gao"
date: "3/22/2017"
output: pdf_document
---

## Step 0: Load the packages, specify directories

```{r}
library(e1071)
library(RTextTools)
library(klaR)
library(pacman)
pacman::p_load(text2vec, dplyr, qlcMatrix, kernlab, knitr)

setwd("~/GitHub/Spr2017-proj4-team7/")
# here replace it with your own path or manually set it in RStudio
# to where this rmd file is located
```

## Step 1: Load and process the data


```{r}
names<-list.files(path = "../data/nameset", pattern = "*.txt")
names<-substr(names,1,nchar(names)-4)

mylist<-list()
for (i in names){

l1 <- data.frame(scan(paste0("../data/nameset/",i,".txt"),
                          what = list(Coauthor = "", Paper = "", Journal = ""),
                          sep=">",quote = NULL, quiet=TRUE),stringsAsFactors=FALSE)
  

# extract canonical author id befor "_"
l1$AuthorID <- sub("_.*","",l1$Coauthor)
# extract paper number under same author between "_" and first whitespace
l1$PaperNO <- sub(".*_(\\w*)\\s.*", "\\1", l1$Coauthor)
# delete "<" in AKumar$Coauthor, you may need to further process the coauthor
# term depending on the method you are using
l1$Coauthor <- gsub("<","",sub("^.*?\\s","", l1$Coauthor))
# delete "<" in AKumar$Paper
l1$Paper <- gsub("<","",l1$Paper)
# add PaperID for furthur use, you may want to combine all the nameset files and 
# then assign the unique ID for all the citations
l1$PaperID <- rownames(l1)

mylist[[i]]<-l1
}

```

## Step 2: Feature design


Let’s first create a vocabulary-based DTM. Here we collect unique terms from all documents and mark each of them with a unique ID using the  `create_vocabulary()` function. We use an iterator to create the vocabulary.
```{r}
mydtm<-list()
for (i in names){
it_train <- itoken(mylist[[i]]$Paper, 
             preprocessor = tolower, 
             tokenizer = word_tokenizer,
             ids = mylist[[i]]$PaperID,
             # turn off progressbar because it won't look nice in rmd
             progressbar = FALSE)
vocab <- create_vocabulary(it_train, stopwords = c("a", "an", "the", "in", "on",
                                                   "at", "of", "above", "under","by","for","and","with","to","about"))
vectorizer <- vocab_vectorizer(vocab)
dtm_train <- create_dtm(it_train, vectorizer)

mydtm[[i]]<-dtm_train
}
#check if any high-frequency stopwords
vocab1=vocab[["vocab"]]
vocab1<-vocab1[order(-terms_counts),]
```

Here, we remove pre-defined stopwords, the words like “a”, “the”, “in”, “I”, “you”, “on”, etc, which do not provide much useful information. 

Now that we have a vocabulary list, we can construct a document-term matrix.
```{r}
vectorizer <- vocab_vectorizer(vocab)
dtm_train <- create_dtm(it_train, vectorizer)

```

Now we have DTM and can check its dimensions.
```{r}
dim(dtm_train)
```


## Step 3: SVM



```{r}

start.time <- Sys.time()
results<-vector()

for (i in names){
  
AuthorID=mylist[[i]]$AuthorID
data=as.matrix(mydtm[[i]])

L=nrow(data)
t=0.5
train_idx<-sample(L,floor(L*t))
train_data<-data[train_idx,]
test_data<-data[-train_idx,]
train_author<-AuthorID[train_idx]
test_author<-AuthorID[-train_idx]
bsvm <- svmlight(train_author ~ train_data,pathsvm="~/svm_light")

svm.pred<- predict(bsvm,as.data.frame(test_data))
er<-sum(svm.pred$class!=test_author)/length(test_author)
results<-c(results,er)
print(paste(i,"error rate is ", round(er*100,2), "%."))




}
end.time <- Sys.time()
time_sclust <- end.time - start.time


```


We can also using hierarchical clustering method under the cosine distance. The intention of using a different clustering method is just to let you know how to compare performance between various methods. 
```{r}
start.time <- Sys.time()
docsdissim <- cosSparse(t(dtm_train_tfidf))
rownames(docsdissim) <- c(1:nrow(dtm_train_tfidf))
colnames(docsdissim) <- c(1:nrow(dtm_train_tfidf))
#compute pairwise cosine similarities using cosSparse function in package qlcMatrix
h <- hclust(as.dist(docsdissim), method = "ward.D")
result_hclust <- cutree(h,length(unique(AKumar$AuthorID)))
end.time <- Sys.time()
time_hclust <- end.time - start.time
table(result_hclust)
```

## Step 4: Evaluation

To evaluate the performance of the method, it is required to calculate the degree of agreement between a set of system-output partitions and a set of true partitions. In general, the agreement between two partitioins is measured for a pair of entities within partitions. The basic unit for which pair-wise agreement is assessed is a pair of entities (authors in our case) which belongs to one of the four cells in the following table (Kang et at.(2009)):

\includegraphics[width=500pt]{matching_matrix.png}

Let $M$ be the set of machine-generated clusters, and $G$ the set of gold standard clusters. Then. in the table, for example, $a$ is the number of pairs of entities that are assigned to the same cluster in each of $M$ and $G$. Hence, $a$ and $d$ are interpreted as agreements, and $b$ and $c$ disagreements. When the table is considered as a confusion matrix for a two-class prediction problem, the standard "Precision", "Recall","F1", and "Accuracy" are defined as follows.

$$
\begin{aligned}
\mbox{Precision} &=\frac{a}{a+b}\\
\mbox{Recall}&=\frac{a}{a+c}\\
\mbox{F1} &=\frac{2\times\mbox{Precision}\times\mbox{Recall}}{\mbox{Precision}+\mbox{Recall}}\\
\mbox{Accuracy}&=\frac{a+d}{a+b+c+d}
\end{aligned}
$$

```{r}
source('.../lib/evaluation_measures.R')
matching_matrix_hclust <- matching_matrix(AKumar$AuthorID,result_hclust)
performance_hclust <- performance_statistics(matching_matrix_hclust)
matching_matrix_sclust <- matching_matrix(AKumar$AuthorID,result_sclust)
performance_sclust <- performance_statistics(matching_matrix_sclust)
compare_df <- data.frame(method=c("sClust","hClust"),
                         precision=c(performance_sclust$precision, performance_hclust$precision),
                         recall=c(performance_sclust$recall, performance_hclust$recall),
                         f1=c(performance_sclust$f1, performance_hclust$f1),
                         accuracy=c(performance_sclust$accuracy, performance_hclust$accuracy),
                         time=c(time_sclust,time_hclust))
kable(compare_df,caption="Comparision of performance for two clustering methods",digits = 2)
```

